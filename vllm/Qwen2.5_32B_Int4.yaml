host: "0.0.0.0"
port: 6379
uvicorn-log-level: "info"
kv-cache-dtype: "fp8"
#calculate-kv-scales: True
served-model-name: "qwen-ns-32b"
gpu_memory_utilization: 0.95
max_model_len: 16384
max_num_seqs: 100
max_num_batched_tokens: 76384

# vllm serve ./huggingface_models/Qwen_Qwen2.5-32B-Instruct-GPTQ-Int4/ --config vllm/Qwen2.5_32B_Int4.yaml
